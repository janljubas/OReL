{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar lander with DQN-style neural function approximator using PyTorch\n",
    "### Christian Igel, 2025\n",
    "\n",
    "If you have suggestions for improvement, [let me know](mailto:igel@diku.dk).\n",
    "\n",
    "I took inspiration from https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb.\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from tqdm.notebook import tqdm  # Progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the game environment (you need the `gym` package):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_visual = gym.make('LunarLander-v3', render_mode=\"human\")\n",
    "action_size = 4\n",
    "state_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test the environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 0\n",
    "for _ in range(test_episodes):\n",
    "    R = 0\n",
    "    state, _ = env_visual.reset()  # Environment starts in a random state, cart and pole are moving\n",
    "    print(\"initial state:\", state)\n",
    "    while True:  # Environment sets \"truncated\" to true after 500 steps \n",
    "        env_visual.render()\n",
    "        state, reward, terminated, truncated, _ = env_visual.step(env_visual.action_space.sample()) #  Take a random action\n",
    "        R += reward  # Accumulate reward\n",
    "        if terminated or truncated:\n",
    "            print(\"return: \", R)\n",
    "            env_visual.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()  # Closes the visualization window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define *Q* network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=10, bias=True):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size, bias)  \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias)  \n",
    "        self.output_layer = nn.Linear(hidden_size + state_size, action_size, bias)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        x = F.tanh(self.fc1(x_input))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = torch.cat((x_input, x), dim=1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure for storing experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define basic constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 400           # Max number of episodes to learn from\n",
    "gamma = 0.99                   # Future reward discount\n",
    "learning_rate = 0.001          # Q-network learning rate\n",
    "tau = .01                      # learning rate for target network\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # Exploration probability at start\n",
    "explore_stop = 0.0001          # Minimum exploration probability \n",
    "decay_rate = 0.05              # Exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # Number of units in each Q-network hidden layer\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # Memory capacity\n",
    "batch_size = 128               # Experience mini-batch size\n",
    "pretrain_length = batch_size   # Number experiences to pretrain the memory\n",
    "\n",
    "log_path = \"/tmp/deep_Q_network\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainQN = QNetwork(hidden_size=hidden_size)\n",
    "print(mainQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the experience memory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env = gym.make('LunarLander-v3')\n",
    "state = env.reset()[0]\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in tqdm(range(pretrain_length)):\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        # The simulation fails, so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train with experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_list = []  # Returns for the individual episodes\n",
    "\n",
    "optimizer = torch.optim.AdamW(mainQN.parameters(), lr=learning_rate) # AdamW uses weight decay by default\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for ep in range(train_episodes):\n",
    "    total_reward = 0  # Return / accumulated rewards\n",
    "    state = env.reset()[0]  # Reset and get initial state\n",
    "    while True:\n",
    "        # Explore or exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*ep) \n",
    "        if explore_p > np.random.rand():\n",
    "            # Pick a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            state_tensor = torch.from_numpy(np.resize(state, (1, state_size)).astype(np.float32))\n",
    "            Qs = mainQN(state_tensor)\n",
    "            action = torch.argmax(Qs).item()\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "        total_reward += reward  # Return / accumulated rewards\n",
    "           \n",
    "        if terminated or truncated:\n",
    "            # Episode ends because of failure, so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "                \n",
    "            print('Episode: {}'.format(ep), 'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss), 'Explore P: {:.4f}'.format(explore_p))\n",
    "            total_reward_list.append((ep, total_reward))\n",
    "                \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            break; # End of episode\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            \n",
    "        # Sample mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        next_states_np = np.array([each[3] for each in batch], dtype=np.float32)\n",
    "        next_states = torch.as_tensor(next_states_np)  # as_tensor does not copy the data\n",
    "        rewards     = torch.as_tensor(np.array([each[2] for each in batch], dtype=np.float32)) \n",
    "        states      = torch.as_tensor(np.array([each[0] for each in batch], dtype=np.float32)) \n",
    "        actions     = torch.as_tensor(np.array([each[1] for each in batch]))\n",
    "              \n",
    "        # Compute Q values for all actions in the new state       \n",
    "        target_Qs = mainQN(next_states)\n",
    "            \n",
    "        # Set target_Qs to 0 for states where episode ended because of failure\n",
    "        episode_ends = (next_states_np == np.zeros(states[0].shape)).all(axis=1)\n",
    "        target_Qs[episode_ends] = torch.zeros(action_size)\n",
    "        \n",
    "        # Compute targets\n",
    "        \n",
    "        # Network learning starts here\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the Q values of the actions taken        \n",
    "        main_Qs = mainQN(states)  # Q values for all action in each state\n",
    "        Q = torch.gather(main_Qs, 1, actions.unsqueeze(-1)).squeeze()  # Only the Q values for the actions taken\n",
    "        \n",
    "        # Gradient-based update\n",
    "        loss = loss_fn(Q, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policy network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mainQN, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average for smoothing plot\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, x[0]*np.ones(N)))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "\n",
    "eps, rews = np.array(total_reward_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "\n",
    "plt.plot(eps, smoothed_rews)\n",
    "plt.grid()\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Accumulated Reward')\n",
    "plt.savefig('deepQ.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate stored policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testQN = torch.load(log_path)\n",
    "\n",
    "test_episodes = 5\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env_visual.reset()[0]\n",
    "    print(\"initial state:\", state)\n",
    "    R = 0\n",
    "    while True:\n",
    "        # Get action from Q-network\n",
    "        # Hm, the following line could perhaps be more elegant ...\n",
    "        state_tensor = torch.from_numpy(np.resize(state, (1, state_size)).astype(np.float32))\n",
    "        Qs = testQN(state_tensor)\n",
    "        action = torch.argmax(Qs).item()\n",
    "            \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, terminated, truncated, _ = env_visual.step(action)\n",
    "        R += reward\n",
    "            \n",
    "        if terminated or truncated:\n",
    "            print(\"reward:\", R)\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
