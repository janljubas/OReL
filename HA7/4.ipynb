{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janljubas/OReL/blob/main/HA7/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original code"
      ],
      "metadata": {
        "id": "wbCk37SNaP6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3kArZ0Q-NfP",
        "outputId": "1907b464-58cc-45e5-afbe-41cf3baa7cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training runs: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it]\n",
            "Training runs: 100%|██████████| 100/100 [14:36<00:00,  8.77s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92):\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Define state indices for clarity\n",
        "        self.states = np.arange(self.n_states)\n",
        "\n",
        "        # Actions: 0 = left (downstream), 1 = right (upstream)\n",
        "        self.actions = [0, 1]\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # P[s, a, s'] gives transition probability from s to s' under action a\n",
        "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using value iteration.\"\"\"\n",
        "        threshold = 1e-10\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                Q_values = np.zeros(self.n_actions)\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    for s_next in range(self.n_states):\n",
        "                        Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = np.max(Q_values)\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = np.zeros(self.n_states, dtype=int)\n",
        "        for s in range(self.n_states):\n",
        "            Q_values = np.zeros(self.n_actions)\n",
        "            for a in range(self.n_actions):\n",
        "                for s_next in range(self.n_states):\n",
        "                    Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "            self.pi_star[s] = np.argmax(Q_values)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = np.random.choice(self.n_states, p=next_state_probs)\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters\n",
        "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
        "        self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, s, a, t):\n",
        "        \"\"\"Calculate the UCB bonus term.\"\"\"\n",
        "        if self.N[s, a] == 0:\n",
        "            return np.inf\n",
        "\n",
        "        # Log term inside sqrt\n",
        "        log_term = np.log(self.n_states * self.n_actions * np.log(t + 1) / self.delta)\n",
        "\n",
        "        # Bonus term\n",
        "        b = np.sqrt((self.H / self.N[s, a]) * log_term)\n",
        "\n",
        "        return b\n",
        "\n",
        "    def get_action(self, state, t):\n",
        "        \"\"\"Select action based on UCB value.\"\"\"\n",
        "        ucb_values = np.zeros(self.n_actions)\n",
        "\n",
        "        for a in range(self.n_actions):\n",
        "            ucb_values[a] = self.Q[state, a] + self.get_ucb_bonus(state, a, t)\n",
        "\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "    def get_greedy_policy(self):\n",
        "        \"\"\"Return the current greedy policy based on Q-values.\"\"\"\n",
        "        return np.argmax(self.Q, axis=1)\n",
        "\n",
        "    def get_policy_value(self, state, policy):\n",
        "        \"\"\"Calculate value of a policy from a given state.\"\"\"\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        # Solve the linear system for value function\n",
        "        threshold = 1e-10\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "\n",
        "                new_v = 0\n",
        "                for s_next in range(self.n_states):\n",
        "                    new_v += self.env.P[s, a, s_next] * (self.env.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = new_v\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        return V[state]\n",
        "\n",
        "    def is_eps_bad(self, state, policy):\n",
        "        \"\"\"Check if policy is ε-bad in given state.\"\"\"\n",
        "        policy_value = self.get_policy_value(state, policy)\n",
        "        return policy_value < self.env.V_star[state] - self.epsilon\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Train UCB-QL algorithm for multiple runs and track ε-bad timesteps.\"\"\"\n",
        "        all_eps_bad_counts = np.zeros((num_runs, self.T))\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset counters and Q-values for this run\n",
        "            self.Q = np.zeros((self.n_states, self.n_actions))\n",
        "            self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "            # Start at a random state\n",
        "            current_state = np.random.randint(0, self.n_states)\n",
        "\n",
        "            # Cumulative count of ε-bad timesteps\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get policy (greedy with respect to Q)\n",
        "                policy = self.get_greedy_policy()\n",
        "\n",
        "                # Check if policy is ε-bad\n",
        "                if self.is_eps_bad(current_state, policy):\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative number of ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # Choose action using UCB rule\n",
        "                action = self.get_action(current_state, t)\n",
        "\n",
        "                # Take action and observe next state and reward\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Update Q-value (using standard Q-learning update)\n",
        "                best_next_q = np.max(self.Q[next_state])\n",
        "                lr = 1.0 / np.sqrt(self.N[current_state, action])  # Learning rate\n",
        "\n",
        "                # Q-learning update\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        return all_eps_bad_counts\n",
        "\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    \"\"\"Plot sample path of n(t) for a single run.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    \"\"\"Plot average n(t) with 95% confidence intervals.\"\"\"\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    # Calculate 95% confidence interval\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over {n_runs} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('average_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000  # 2 million\n",
        "\n",
        "    # Create environment\n",
        "    env = RiverSwim(gamma=gamma)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Reduced T for testing - comment this out for full run\n",
        "    # T = 20000\n",
        "    # agent.T = T\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    single_run_counts = agent.train(num_runs=1)[0]\n",
        "    plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=100)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V2"
      ],
      "metadata": {
        "id": "0FbEFIQHaTlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        # Set device (GPU if available, otherwise CPU)\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Use torch tensors and move to device\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # Use torch tensors and move to device\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using value iteration.\"\"\"\n",
        "        threshold = 1e-10\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                Q_values = torch.zeros(self.n_actions, device=self.device)\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    for s_next in range(self.n_states):\n",
        "                        Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = torch.max(Q_values)\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = torch.zeros(self.n_states, dtype=torch.long, device=self.device)\n",
        "        for s in range(self.n_states):\n",
        "            Q_values = torch.zeros(self.n_actions, device=self.device)\n",
        "            for a in range(self.n_actions):\n",
        "                for s_next in range(self.n_states):\n",
        "                    Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "            self.pi_star[s] = torch.argmax(Q_values)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters using torch tensors\n",
        "        self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, s, a, t):\n",
        "        \"\"\"Calculate the UCB bonus term.\"\"\"\n",
        "        if self.N[s, a] == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        # Log term inside sqrt\n",
        "        log_term = torch.log(torch.tensor(self.n_states * self.n_actions * torch.log(torch.tensor(t + 1)) / self.delta))\n",
        "\n",
        "        # Bonus term\n",
        "        b = torch.sqrt((self.H / self.N[s, a]) * log_term)\n",
        "\n",
        "        return b.item()\n",
        "\n",
        "    def get_action(self, state, t):\n",
        "        \"\"\"Select action based on UCB value.\"\"\"\n",
        "        ucb_values = torch.zeros(self.n_actions, device=self.device)\n",
        "\n",
        "        for a in range(self.n_actions):\n",
        "            ucb_values[a] = self.Q[state, a] + self.get_ucb_bonus(state, a, t)\n",
        "\n",
        "        return torch.argmax(ucb_values).item()\n",
        "\n",
        "    def get_greedy_policy(self):\n",
        "        \"\"\"Return the current greedy policy based on Q-values.\"\"\"\n",
        "        return torch.argmax(self.Q, dim=1)\n",
        "\n",
        "    def get_policy_value(self, state, policy):\n",
        "        \"\"\"Calculate value of a policy from a given state.\"\"\"\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        # Solve the linear system for value function\n",
        "        threshold = 1e-10\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "\n",
        "                new_v = 0\n",
        "                for s_next in range(self.n_states):\n",
        "                    new_v += self.env.P[s, a, s_next] * (self.env.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = new_v\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        return V[state].item()\n",
        "\n",
        "    def is_eps_bad(self, state, policy):\n",
        "        \"\"\"Check if policy is ε-bad in given state.\"\"\"\n",
        "        policy_value = self.get_policy_value(state, policy)\n",
        "        return policy_value < self.env.V_star[state] - self.epsilon\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Train UCB-QL algorithm for multiple runs and track ε-bad timesteps.\"\"\"\n",
        "        all_eps_bad_counts = torch.zeros((num_runs, self.T), device=self.device)\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset counters and Q-values for this run\n",
        "            self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "            self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "            # Start at a random state\n",
        "            current_state = torch.randint(0, self.n_states, (1,), device=self.device).item()\n",
        "\n",
        "            # Cumulative count of ε-bad timesteps\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get policy (greedy with respect to Q)\n",
        "                policy = self.get_greedy_policy()\n",
        "\n",
        "                # Check if policy is ε-bad\n",
        "                if self.is_eps_bad(current_state, policy):\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative number of ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # Choose action using UCB rule\n",
        "                action = self.get_action(current_state, t)\n",
        "\n",
        "                # Take action and observe next state and reward\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Update Q-value (using standard Q-learning update)\n",
        "                best_next_q = torch.max(self.Q[next_state])\n",
        "                lr = 1.0 / torch.sqrt(self.N[current_state, action].float())  # Learning rate\n",
        "\n",
        "                # Q-learning update\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        # Move results back to CPU for plotting\n",
        "        return all_eps_bad_counts.cpu().numpy()\n",
        "\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    \"\"\"Plot sample path of n(t) for a single run.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    \"\"\"Plot average n(t) with 95% confidence intervals.\"\"\"\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    # Calculate 95% confidence interval\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over {n_runs} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('average_run_V2.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Check and print GPU availability\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000\n",
        "\n",
        "    # Create environment with GPU support\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = RiverSwim(gamma=gamma, device=device)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    # single_run_counts = agent.train(num_runs=1)[0]\n",
        "    # plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=100)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6OD-WuTA11x",
        "outputId": "bb15cd2d-369b-47c1-d2fc-6707817bd424"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining runs:   0%|          | 0/100 [00:00<?, ?it/s]<ipython-input-6-7504e47ca934>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_term = torch.log(torch.tensor(self.n_states * self.n_actions * torch.log(torch.tensor(t + 1)) / self.delta))\n",
            "Training runs: 100%|██████████| 100/100 [12:59<00:00,  7.80s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V3"
      ],
      "metadata": {
        "id": "vLKssd9waWmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        # Set device (GPU if available, otherwise CPU)\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Use torch tensors and move to device\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # Use torch tensors and move to device\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using vectorized value iteration.\"\"\"\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        # Vectorized Q-value calculation\n",
        "        while True:\n",
        "            # Compute Q-values for all states and actions in one go\n",
        "            # Dimensions: (states, actions)\n",
        "            Q_values = torch.einsum('sas,sas->sa', self.P, self.R + self.gamma * V)\n",
        "\n",
        "            # Find max Q-value for each state\n",
        "            new_V = Q_values.max(dim=1).values\n",
        "\n",
        "            # Check convergence\n",
        "            delta = torch.abs(new_V - V).max()\n",
        "\n",
        "            # Update V\n",
        "            V = new_V\n",
        "\n",
        "            if delta < 1e-3:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = Q_values.argmax(dim=1)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters using torch tensors\n",
        "        self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, t):\n",
        "        \"\"\"Calculate UCB bonus for all state-action pairs.\"\"\"\n",
        "        # Avoid division by zero\n",
        "        safe_N = torch.clamp(self.N, min=1)\n",
        "\n",
        "        # Log term\n",
        "        log_term = torch.log(\n",
        "            self.n_states * self.n_actions * torch.log(torch.tensor(t + 1, device=self.device)) / self.delta\n",
        "        )\n",
        "\n",
        "        # Vectorized bonus calculation\n",
        "        bonus = torch.sqrt((self.H / safe_N) * log_term)\n",
        "        return bonus\n",
        "\n",
        "    def get_policy_value(self, policy):\n",
        "        \"\"\"Calculate value of a policy using matrix operations.\"\"\"\n",
        "        # Create identity matrix to help with policy evaluation\n",
        "        I = torch.eye(self.n_states, device=self.device)\n",
        "\n",
        "        # Construct policy transition matrix and reward matrix\n",
        "        P_pi = torch.zeros((self.n_states, self.n_states), device=self.device)\n",
        "        R_pi = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        for s in range(self.n_states):\n",
        "            a = policy[s]\n",
        "            P_pi[s] = self.env.P[s, a]\n",
        "            R_pi[s] = torch.sum(self.env.P[s, a] * self.env.R[s, a])\n",
        "\n",
        "        # Solve (I - γP)V = R using matrix inverse\n",
        "        V = torch.linalg.solve(I - self.gamma * P_pi, R_pi)\n",
        "\n",
        "        return V\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Vectorized training of UCB-QL algorithm.\"\"\"\n",
        "        # Preallocate tensor for tracking ε-bad timesteps\n",
        "        all_eps_bad_counts = torch.zeros((num_runs, self.T), device=self.device)\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset for this run\n",
        "            self.Q.zero_()\n",
        "            self.N.zero_()\n",
        "\n",
        "            # Start at random state\n",
        "            current_state = torch.randint(0, self.n_states, (1,), device=self.device).item()\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get current greedy policy\n",
        "                policy = self.Q.argmax(dim=1)\n",
        "\n",
        "                # Vectorized policy value calculation\n",
        "                policy_values = self.get_policy_value(policy)\n",
        "\n",
        "                # Check if policy is ε-bad for current state\n",
        "                is_eps_bad = policy_values[current_state] < self.env.V_star[current_state] - self.epsilon\n",
        "                if is_eps_bad:\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # UCB action selection (includes bonus)\n",
        "                ucb_values = self.Q[current_state] + self.get_ucb_bonus(t)[current_state]\n",
        "                action = ucb_values.argmax().item()\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Learning rate\n",
        "                lr = 1.0 / torch.sqrt(self.N[current_state, action].float())\n",
        "\n",
        "                # Q-learning update\n",
        "                best_next_q = self.Q[next_state].max()\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        # Move results back to CPU for plotting\n",
        "        return all_eps_bad_counts.cpu().numpy()\n",
        "\n",
        "# Plotting functions remain the same as in previous version\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run_V3.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over 100 runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('FINAL_RUN_V3.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Check and print GPU availability\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000000\n",
        "\n",
        "    # Create environment with GPU support\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = RiverSwim(gamma=gamma, device=device)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    # single_run_counts = agent.train(num_runs=1)[0]\n",
        "    # plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=10)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk74_9fhaXp7",
        "outputId": "e4ccf4b2-fe8d-4ac6-a113-be62cb4d9536"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training runs: 100%|██████████| 10/10 [05:21<00:00, 32.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V4"
      ],
      "metadata": {
        "id": "mTuBnVCnw_DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "        self.initialize_mdp()\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:\n",
        "                self.P[s, 0, s] = 1.0\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7\n",
        "                self.P[s, 0, s] = 0.3\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:\n",
        "                self.P[s, 1, s] = 0.7\n",
        "                self.P[s, 1, s-1] = 0.3\n",
        "                self.R[s, 1, s] = 1.0\n",
        "            elif s == 0:\n",
        "                self.P[s, 1, s+1] = 0.6\n",
        "                self.P[s, 1, s] = 0.4\n",
        "                self.R[s, 1, s] = 0.05\n",
        "            else:\n",
        "                self.P[s, 1, s+1] = 0.6\n",
        "                self.P[s, 1, s] = 0.1\n",
        "                self.P[s, 1, s-1] = 0.3\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "        while True:\n",
        "            Q = torch.einsum('sas,sas->sa', self.P, self.R + self.gamma * V)\n",
        "            new_V = Q.max(dim=1).values\n",
        "            if torch.abs(new_V - V).max() < 1e-3:\n",
        "                break\n",
        "            V = new_V\n",
        "        self.V_star = V\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2_000_000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize parameters as per paper\n",
        "        self.H = int(np.ceil(np.log(3 / (epsilon * (1 - gamma))) / (1 - gamma)))  # Correct H\n",
        "        self.Q = torch.full((self.n_states, self.n_actions), 1/(1 - gamma), device=self.device)  # Optimistic initialization\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "    def get_ucb_bonus(self, t):\n",
        "        \"\"\"Vectorized UCB bonus calculation as per Wang et al.\"\"\"\n",
        "        safe_N = torch.clamp(self.N, min=1)\n",
        "        log_term = torch.log((self.n_states * self.n_actions * torch.log(torch.tensor(t + 1, device=self.device)) / self.delta))\n",
        "        return torch.sqrt((self.H / safe_N) * log_term)\n",
        "\n",
        "    def train(self, num_runs=100):\n",
        "        all_eps_bad = torch.zeros((num_runs, self.T // 1000), device=self.device)  # Downsampled storage\n",
        "\n",
        "        for run in range(num_runs):\n",
        "            self.Q.fill_(1/(1 - self.gamma))  # Reset Q\n",
        "            self.N.zero_()\n",
        "            current_state = torch.randint(0, self.n_states, (1,)).item()\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in tqdm(range(self.T), desc=f\"Run {run+1}/{num_runs}\", leave=False):\n",
        "                # Get UCB values for current state\n",
        "                ucb = self.Q[current_state] + self.get_ucb_bonus(t)[current_state]\n",
        "                action = ucb.argmax().item()\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "                self.N[current_state, action] += 1\n",
        "                k = self.N[current_state, action]\n",
        "\n",
        "                # Q-learning update with correct learning rate\n",
        "                lr = (self.H + 1) / (self.H + k)  # Paper's α_k\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * self.Q[next_state].max() - self.Q[current_state, action])\n",
        "\n",
        "                # Check ε-bad using UCB values (no full policy evaluation)\n",
        "                current_V_ucb = self.Q[current_state].max() + self.get_ucb_bonus(t)[current_state].max()\n",
        "                if current_V_ucb < self.env.V_star[current_state] - self.epsilon:\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Downsample: store every 1000 steps\n",
        "                if t % 1000 == 0:\n",
        "                    all_eps_bad[run, t // 1000] = eps_bad_count\n",
        "\n",
        "                current_state = next_state\n",
        "\n",
        "        return all_eps_bad.cpu().numpy()\n",
        "\n",
        "def plot_results(data, T=2_000_000):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(0, T, 1000) // 1000  # Plot in units of 1000 steps\n",
        "    plt.plot(x, np.mean(data, axis=0), label='Average n(t)')\n",
        "    plt.fill_between(x,\n",
        "                     np.mean(data, axis=0) - 1.96 * np.std(data, axis=0)/np.sqrt(data.shape[0]),\n",
        "                     np.mean(data, axis=0) + 1.96 * np.std(data, axis=0)/np.sqrt(data.shape[0]),\n",
        "                     alpha=0.3, label='95% CI')\n",
        "    plt.xlabel('Timestep (t, in thousands)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps')\n",
        "    plt.title(f'UCB-QL Performance (T={T//1_000}K, {data.shape[0]} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig('corrected_results.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "    env = RiverSwim(gamma=0.92, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    agent = UCBQLearning(env, T=2000000)\n",
        "    data = agent.train(num_runs=10)\n",
        "    plot_results(data, 2000000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VGztaQ8xAa3",
        "outputId": "d2e22330-c6d0-483c-b3ab-ad1605d5484c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V5"
      ],
      "metadata": {
        "id": "hZ6eu8GqjnEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92):\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Precompute transition probabilities and rewards\n",
        "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        # Compute optimal values using value iteration\n",
        "        self.compute_optimal_values()\n",
        "\n",
        "    def compute_optimal_values(self):\n",
        "        # Value iteration to compute optimal values\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "\n",
        "        while True:\n",
        "            V_new = np.zeros(self.n_states)\n",
        "            for s in range(self.n_states):\n",
        "\n",
        "                # Consider both actions\n",
        "                values = []\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    # Expected value for this action\n",
        "                    v = 0\n",
        "                    if s == 4 and a == 1:  # Rightmost state, upstream action\n",
        "                        v = 1 + self.gamma * V[4]\n",
        "                    elif s == 0 and a == 1:  # Leftmost state, upstream action\n",
        "                        # Mostly stay, small chance to move right\n",
        "                        v = 0.05 + 0.6 * self.gamma * V[1]\n",
        "                    else:\n",
        "                        # General transition probabilities\n",
        "                        if a == 0:  # Left/downstream action\n",
        "                            v = 0.7 * self.gamma * V[max(0, s-1)] + 0.3 * self.gamma * V[s]\n",
        "                        else:  # Right/upstream action\n",
        "                            if s == self.n_states - 2:\n",
        "                                v = 0.6 * self.gamma * V[s+1] + 0.1 * self.gamma * V[s] + 0.3 * self.gamma * V[s-1]\n",
        "                            else:\n",
        "                                v = 0.6 * self.gamma * V[s+1] + 0.1 * self.gamma * V[s] + 0.3 * self.gamma * V[s-1]\n",
        "                    values.append(v)\n",
        "                V_new[s] = max(values)\n",
        "\n",
        "            # Check convergence\n",
        "            if np.max(np.abs(V - V_new)) < 1e-2:\n",
        "                break\n",
        "            V = V_new\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "    def reset(self):\n",
        "        # Uniform distribution over states\n",
        "        return np.random.randint(0, self.n_states)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Stochastic transitions\n",
        "        if state == 4 and action == 1:  # Rightmost state, upstream action\n",
        "            return 4, 1\n",
        "\n",
        "        # Transition probabilities\n",
        "        if action == 0:  # Left/downstream\n",
        "            trans_probs = [0.7, 0.3] if state > 0 else [0, 1]\n",
        "            next_state = max(0, state-1) if np.random.random() < 0.7 else state\n",
        "            reward = 0\n",
        "        else:  # Right/upstream\n",
        "            if state == 0:\n",
        "                trans_probs = [0.6, 0.4]\n",
        "                next_state = 1 if np.random.random() < 0.6 else 0\n",
        "                reward = 0.05\n",
        "            elif state == 4:\n",
        "                trans_probs = [0.3, 0.7]\n",
        "                next_state = 3 if np.random.random() < 0.3 else 4\n",
        "                reward = 1\n",
        "            else:\n",
        "                trans_probs = [0.3, 0.1, 0.6]\n",
        "                r = np.random.random()\n",
        "                if r < 0.3:\n",
        "                    next_state = state-1\n",
        "                elif r < 0.4:\n",
        "                    next_state = state\n",
        "                else:\n",
        "                    next_state = state+1\n",
        "                reward = 0\n",
        "\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQL:\n",
        "    def __init__(self, n_states, n_actions, gamma, epsilon, delta, T):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.T = T\n",
        "\n",
        "        # Horizon calculation\n",
        "        self.H = int(1 / (1 - gamma) * np.log(1 / epsilon))\n",
        "\n",
        "        # Initialize Q-values optimistically\n",
        "        self.Q = np.ones((n_states, n_actions)) / (1 - gamma)\n",
        "        self.N = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def get_confidence_bonus(self, state, action, t):\n",
        "        # UCB bonus as per the paper\n",
        "        N_sa = max(1, self.N[state, action])\n",
        "        log_term = np.log(self.n_states * self.n_actions * np.log(t + 1) / self.delta)\n",
        "        return np.sqrt(self.H / N_sa * log_term)\n",
        "\n",
        "    def choose_action(self, state, t):\n",
        "        # UCB action selection\n",
        "        bonus_values = self.Q[state, :] + [self.get_confidence_bonus(state, a, t)\n",
        "                                           for a in range(self.n_actions)]\n",
        "        return np.argmax(bonus_values)\n",
        "\n",
        "    def update(self, state, action, next_state, reward, t):\n",
        "        # Update visit count\n",
        "        self.N[state, action] += 1\n",
        "        k = self.N[state, action]\n",
        "\n",
        "        # Learning rate as suggested in the paper\n",
        "        lr = (self.H + 1) / (self.H + k)\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state, action] += lr * (reward + self.gamma * np.max(self.Q[next_state, :]) -\n",
        "                                       self.Q[state, action])\n",
        "\n",
        "def run_ucb_ql_experiment(T=200000, n_runs=50):\n",
        "    # Experiment parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "\n",
        "    # store cumulative epsilon-bad time steps\n",
        "    all_n_t = []\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "\n",
        "      env = RiverSwim(gamma)\n",
        "      agent = UCBQL(n_states=5, n_actions=2, gamma=gamma,\n",
        "                    epsilon=epsilon, delta=delta, T=T)\n",
        "\n",
        "      # Track epsilon-bad time steps\n",
        "      n_t = np.zeros(T)\n",
        "      state = env.reset()\n",
        "\n",
        "      # so this for loop is one run\n",
        "      for t in range(T):\n",
        "          # Choose action with UCB exploration\n",
        "          action = agent.choose_action(state, t+1)\n",
        "\n",
        "          # Take step\n",
        "          next_state, reward = env.step(state, action)\n",
        "\n",
        "          # Check if time step is epsilon-bad\n",
        "          # Compare UCB Q-value estimate with true optimal value\n",
        "          ucb_value = agent.Q[state, action] + agent.get_confidence_bonus(state, action, t+1)\n",
        "          if ucb_value < env.V_star[state] - epsilon:\n",
        "              n_t[t] = 1\n",
        "\n",
        "          # Update Q-values\n",
        "          agent.update(state, action, next_state, reward, t+1)\n",
        "\n",
        "          # Move to next state\n",
        "          state = next_state\n",
        "\n",
        "      # now we can store the cumulative epsilon-bad time steps of this run in the collective data\n",
        "      all_n_t.append(np.cumsum(n_t))\n",
        "\n",
        "\n",
        "\n",
        "    # after all n_runs of runs are complete, compute mean and confidence intervals\n",
        "    mean_n_t = np.mean(all_n_t, axis=0)\n",
        "    std_n_t = np.std(all_n_t, axis=0)\n",
        "    confidence_interval = 1.96 * std_n_t / np.sqrt(n_runs)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(mean_n_t, label='Mean Cumulative ε-Bad Time Steps')\n",
        "    plt.fill_between(range(T),\n",
        "                     mean_n_t - confidence_interval,\n",
        "                     mean_n_t + confidence_interval,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "    plt.title('UCB Q-Learning: Cumulative ε-Bad Time Steps')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('n(t): Cumulative ε-Bad Time Steps')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mean_n_t, confidence_interval\n",
        "\n",
        "# Run the experiment\n",
        "mean_n_t, confidence_interval = run_ucb_ql_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "HhK7LtL3joLL",
        "outputId": "ef1f6fc3-f21c-491c-d842-1aa16d4992e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}