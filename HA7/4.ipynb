{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "class RiverSwim:\n",
    "    def __init__(self, gamma=0.92):\n",
    "        self.n_states = 5\n",
    "        self.n_actions = 2\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Define state indices for clarity\n",
    "        self.states = np.arange(self.n_states)\n",
    "        \n",
    "        # Actions: 0 = left (downstream), 1 = right (upstream)\n",
    "        self.actions = [0, 1]\n",
    "        \n",
    "        # Initialize transition probabilities and rewards\n",
    "        self.initialize_mdp()\n",
    "        \n",
    "        # Calculate optimal values\n",
    "        self.calculate_optimal_values()\n",
    "    \n",
    "    def initialize_mdp(self):\n",
    "        # P[s, a, s'] gives transition probability from s to s' under action a\n",
    "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        # Action 0 (left/downstream)\n",
    "        for s in range(self.n_states):\n",
    "            if s == 0:  # leftmost state\n",
    "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
    "            else:\n",
    "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
    "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
    "        \n",
    "        # Action 1 (right/upstream)\n",
    "        for s in range(self.n_states):\n",
    "            if s == self.n_states - 1:  # rightmost state\n",
    "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
    "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
    "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
    "            elif s == 0:  # leftmost state\n",
    "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
    "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
    "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
    "            else:  # middle states\n",
    "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
    "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
    "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
    "    \n",
    "    def calculate_optimal_values(self):\n",
    "        \"\"\"Calculate optimal state values using value iteration.\"\"\"\n",
    "        threshold = 1e-10\n",
    "        V = np.zeros(self.n_states)\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.n_states):\n",
    "                v = V[s]\n",
    "                Q_values = np.zeros(self.n_actions)\n",
    "                \n",
    "                for a in range(self.n_actions):\n",
    "                    for s_next in range(self.n_states):\n",
    "                        Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
    "                \n",
    "                V[s] = np.max(Q_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            \n",
    "            if delta < threshold:\n",
    "                break\n",
    "        \n",
    "        self.V_star = V\n",
    "        \n",
    "        # Calculate optimal policy\n",
    "        self.pi_star = np.zeros(self.n_states, dtype=int)\n",
    "        for s in range(self.n_states):\n",
    "            Q_values = np.zeros(self.n_actions)\n",
    "            for a in range(self.n_actions):\n",
    "                for s_next in range(self.n_states):\n",
    "                    Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
    "            self.pi_star[s] = np.argmax(Q_values)\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
    "        next_state_probs = self.P[state, action]\n",
    "        next_state = np.random.choice(self.n_states, p=next_state_probs)\n",
    "        reward = self.R[state, action, next_state]\n",
    "        return next_state, reward\n",
    "\n",
    "class UCBQLearning:\n",
    "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "        self.T = T\n",
    "        \n",
    "        self.n_states = env.n_states\n",
    "        self.n_actions = env.n_actions\n",
    "        \n",
    "        # Initialize Q-values, visit counts, and bonus parameters\n",
    "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "        self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
    "        \n",
    "        # Set horizon based on discount and epsilon as suggested\n",
    "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
    "    \n",
    "    def get_ucb_bonus(self, s, a, t):\n",
    "        \"\"\"Calculate the UCB bonus term.\"\"\"\n",
    "        if self.N[s, a] == 0:\n",
    "            return np.inf\n",
    "        \n",
    "        # Log term inside sqrt\n",
    "        log_term = np.log(self.n_states * self.n_actions * np.log(t + 1) / self.delta)\n",
    "        \n",
    "        # Bonus term \n",
    "        b = np.sqrt((self.H / self.N[s, a]) * log_term)\n",
    "        \n",
    "        return b\n",
    "    \n",
    "    def get_action(self, state, t):\n",
    "        \"\"\"Select action based on UCB value.\"\"\"\n",
    "        ucb_values = np.zeros(self.n_actions)\n",
    "        \n",
    "        for a in range(self.n_actions):\n",
    "            ucb_values[a] = self.Q[state, a] + self.get_ucb_bonus(state, a, t)\n",
    "        \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def get_greedy_policy(self):\n",
    "        \"\"\"Return the current greedy policy based on Q-values.\"\"\"\n",
    "        return np.argmax(self.Q, axis=1)\n",
    "    \n",
    "    def get_policy_value(self, state, policy):\n",
    "        \"\"\"Calculate value of a policy from a given state.\"\"\"\n",
    "        V = np.zeros(self.n_states)\n",
    "        \n",
    "        # Solve the linear system for value function\n",
    "        threshold = 1e-10\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.n_states):\n",
    "                v = V[s]\n",
    "                a = policy[s]\n",
    "                \n",
    "                new_v = 0\n",
    "                for s_next in range(self.n_states):\n",
    "                    new_v += self.env.P[s, a, s_next] * (self.env.R[s, a, s_next] + self.gamma * V[s_next])\n",
    "                \n",
    "                V[s] = new_v\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            \n",
    "            if delta < threshold:\n",
    "                break\n",
    "        \n",
    "        return V[state]\n",
    "    \n",
    "    def is_eps_bad(self, state, policy):\n",
    "        \"\"\"Check if policy is ε-bad in given state.\"\"\"\n",
    "        policy_value = self.get_policy_value(state, policy)\n",
    "        return policy_value < self.env.V_star[state] - self.epsilon\n",
    "    \n",
    "    def train(self, num_runs=1):\n",
    "        \"\"\"Train UCB-QL algorithm for multiple runs and track ε-bad timesteps.\"\"\"\n",
    "        all_eps_bad_counts = np.zeros((num_runs, self.T))\n",
    "        \n",
    "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
    "            # Reset counters and Q-values for this run\n",
    "            self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "            self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
    "            \n",
    "            # Start at a random state\n",
    "            current_state = np.random.randint(0, self.n_states)\n",
    "            \n",
    "            # Cumulative count of ε-bad timesteps\n",
    "            eps_bad_count = 0\n",
    "            \n",
    "            for t in range(self.T):\n",
    "                # Get policy (greedy with respect to Q)\n",
    "                policy = self.get_greedy_policy()\n",
    "                \n",
    "                # Check if policy is ε-bad\n",
    "                if self.is_eps_bad(current_state, policy):\n",
    "                    eps_bad_count += 1\n",
    "                \n",
    "                # Record cumulative number of ε-bad timesteps\n",
    "                all_eps_bad_counts[run, t] = eps_bad_count\n",
    "                \n",
    "                # Choose action using UCB rule\n",
    "                action = self.get_action(current_state, t)\n",
    "                \n",
    "                # Take action and observe next state and reward\n",
    "                next_state, reward = self.env.step(current_state, action)\n",
    "                \n",
    "                # Update visit count\n",
    "                self.N[current_state, action] += 1\n",
    "                \n",
    "                # Update Q-value (using standard Q-learning update)\n",
    "                best_next_q = np.max(self.Q[next_state])\n",
    "                lr = 1.0 / np.sqrt(self.N[current_state, action])  # Learning rate\n",
    "                \n",
    "                # Q-learning update\n",
    "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
    "                \n",
    "                # Move to next state\n",
    "                current_state = next_state\n",
    "        \n",
    "        return all_eps_bad_counts\n",
    "\n",
    "def plot_single_run(eps_bad_counts):\n",
    "    \"\"\"Plot sample path of n(t) for a single run.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eps_bad_counts)\n",
    "    plt.xlabel('Timestep (t)')\n",
    "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
    "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('single_run.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_average_with_ci(all_eps_bad_counts):\n",
    "    \"\"\"Plot average n(t) with 95% confidence intervals.\"\"\"\n",
    "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
    "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
    "    n_runs = all_eps_bad_counts.shape[0]\n",
    "    \n",
    "    # Calculate 95% confidence interval\n",
    "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mean_counts, label='Average n(t)')\n",
    "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95, \n",
    "                     alpha=0.3, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.xlabel('Timestep (t)')\n",
    "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
    "    plt.title(f'Average Cumulative ε-bad Timesteps (over {n_runs} runs)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('average_run.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Set parameters\n",
    "    gamma = 0.92\n",
    "    epsilon = 0.13\n",
    "    delta = 0.05\n",
    "    T = 2000  # 2 million\n",
    "    \n",
    "    # Create environment\n",
    "    env = RiverSwim(gamma=gamma)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
    "    \n",
    "    # Reduced T for testing - comment this out for full run\n",
    "    # T = 20000\n",
    "    # agent.T = T\n",
    "    \n",
    "    # Run single iteration for part (i)\n",
    "    single_run_counts = agent.train(num_runs=1)[0]\n",
    "    plot_single_run(single_run_counts)\n",
    "    \n",
    "    # Run multiple iterations for part (ii)\n",
    "    all_eps_bad_counts = agent.train(num_runs=100)\n",
    "    plot_average_with_ci(all_eps_bad_counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
