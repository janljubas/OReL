{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janljubas/OReL/blob/main/HA7/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original code"
      ],
      "metadata": {
        "id": "wbCk37SNaP6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "n3kArZ0Q-NfP",
        "outputId": "1907b464-58cc-45e5-afbe-41cf3baa7cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training runs: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it]\n",
            "Training runs: 100%|██████████| 100/100 [14:36<00:00,  8.77s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92):\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Define state indices for clarity\n",
        "        self.states = np.arange(self.n_states)\n",
        "\n",
        "        # Actions: 0 = left (downstream), 1 = right (upstream)\n",
        "        self.actions = [0, 1]\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # P[s, a, s'] gives transition probability from s to s' under action a\n",
        "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using value iteration.\"\"\"\n",
        "        threshold = 1e-10\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                Q_values = np.zeros(self.n_actions)\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    for s_next in range(self.n_states):\n",
        "                        Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = np.max(Q_values)\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = np.zeros(self.n_states, dtype=int)\n",
        "        for s in range(self.n_states):\n",
        "            Q_values = np.zeros(self.n_actions)\n",
        "            for a in range(self.n_actions):\n",
        "                for s_next in range(self.n_states):\n",
        "                    Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "            self.pi_star[s] = np.argmax(Q_values)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = np.random.choice(self.n_states, p=next_state_probs)\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters\n",
        "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
        "        self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, s, a, t):\n",
        "        \"\"\"Calculate the UCB bonus term.\"\"\"\n",
        "        if self.N[s, a] == 0:\n",
        "            return np.inf\n",
        "\n",
        "        # Log term inside sqrt\n",
        "        log_term = np.log(self.n_states * self.n_actions * np.log(t + 1) / self.delta)\n",
        "\n",
        "        # Bonus term\n",
        "        b = np.sqrt((self.H / self.N[s, a]) * log_term)\n",
        "\n",
        "        return b\n",
        "\n",
        "    def get_action(self, state, t):\n",
        "        \"\"\"Select action based on UCB value.\"\"\"\n",
        "        ucb_values = np.zeros(self.n_actions)\n",
        "\n",
        "        for a in range(self.n_actions):\n",
        "            ucb_values[a] = self.Q[state, a] + self.get_ucb_bonus(state, a, t)\n",
        "\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "    def get_greedy_policy(self):\n",
        "        \"\"\"Return the current greedy policy based on Q-values.\"\"\"\n",
        "        return np.argmax(self.Q, axis=1)\n",
        "\n",
        "    def get_policy_value(self, state, policy):\n",
        "        \"\"\"Calculate value of a policy from a given state.\"\"\"\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        # Solve the linear system for value function\n",
        "        threshold = 1e-10\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "\n",
        "                new_v = 0\n",
        "                for s_next in range(self.n_states):\n",
        "                    new_v += self.env.P[s, a, s_next] * (self.env.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = new_v\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        return V[state]\n",
        "\n",
        "    def is_eps_bad(self, state, policy):\n",
        "        \"\"\"Check if policy is ε-bad in given state.\"\"\"\n",
        "        policy_value = self.get_policy_value(state, policy)\n",
        "        return policy_value < self.env.V_star[state] - self.epsilon\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Train UCB-QL algorithm for multiple runs and track ε-bad timesteps.\"\"\"\n",
        "        all_eps_bad_counts = np.zeros((num_runs, self.T))\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset counters and Q-values for this run\n",
        "            self.Q = np.zeros((self.n_states, self.n_actions))\n",
        "            self.N = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "            # Start at a random state\n",
        "            current_state = np.random.randint(0, self.n_states)\n",
        "\n",
        "            # Cumulative count of ε-bad timesteps\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get policy (greedy with respect to Q)\n",
        "                policy = self.get_greedy_policy()\n",
        "\n",
        "                # Check if policy is ε-bad\n",
        "                if self.is_eps_bad(current_state, policy):\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative number of ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # Choose action using UCB rule\n",
        "                action = self.get_action(current_state, t)\n",
        "\n",
        "                # Take action and observe next state and reward\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Update Q-value (using standard Q-learning update)\n",
        "                best_next_q = np.max(self.Q[next_state])\n",
        "                lr = 1.0 / np.sqrt(self.N[current_state, action])  # Learning rate\n",
        "\n",
        "                # Q-learning update\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        return all_eps_bad_counts\n",
        "\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    \"\"\"Plot sample path of n(t) for a single run.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    \"\"\"Plot average n(t) with 95% confidence intervals.\"\"\"\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    # Calculate 95% confidence interval\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over {n_runs} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('average_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000  # 2 million\n",
        "\n",
        "    # Create environment\n",
        "    env = RiverSwim(gamma=gamma)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Reduced T for testing - comment this out for full run\n",
        "    # T = 20000\n",
        "    # agent.T = T\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    single_run_counts = agent.train(num_runs=1)[0]\n",
        "    plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=100)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V2"
      ],
      "metadata": {
        "id": "0FbEFIQHaTlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        # Set device (GPU if available, otherwise CPU)\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Use torch tensors and move to device\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # Use torch tensors and move to device\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using value iteration.\"\"\"\n",
        "        threshold = 1e-10\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                Q_values = torch.zeros(self.n_actions, device=self.device)\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    for s_next in range(self.n_states):\n",
        "                        Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = torch.max(Q_values)\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = torch.zeros(self.n_states, dtype=torch.long, device=self.device)\n",
        "        for s in range(self.n_states):\n",
        "            Q_values = torch.zeros(self.n_actions, device=self.device)\n",
        "            for a in range(self.n_actions):\n",
        "                for s_next in range(self.n_states):\n",
        "                    Q_values[a] += self.P[s, a, s_next] * (self.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "            self.pi_star[s] = torch.argmax(Q_values)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters using torch tensors\n",
        "        self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, s, a, t):\n",
        "        \"\"\"Calculate the UCB bonus term.\"\"\"\n",
        "        if self.N[s, a] == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        # Log term inside sqrt\n",
        "        log_term = torch.log(torch.tensor(self.n_states * self.n_actions * torch.log(torch.tensor(t + 1)) / self.delta))\n",
        "\n",
        "        # Bonus term\n",
        "        b = torch.sqrt((self.H / self.N[s, a]) * log_term)\n",
        "\n",
        "        return b.item()\n",
        "\n",
        "    def get_action(self, state, t):\n",
        "        \"\"\"Select action based on UCB value.\"\"\"\n",
        "        ucb_values = torch.zeros(self.n_actions, device=self.device)\n",
        "\n",
        "        for a in range(self.n_actions):\n",
        "            ucb_values[a] = self.Q[state, a] + self.get_ucb_bonus(state, a, t)\n",
        "\n",
        "        return torch.argmax(ucb_values).item()\n",
        "\n",
        "    def get_greedy_policy(self):\n",
        "        \"\"\"Return the current greedy policy based on Q-values.\"\"\"\n",
        "        return torch.argmax(self.Q, dim=1)\n",
        "\n",
        "    def get_policy_value(self, state, policy):\n",
        "        \"\"\"Calculate value of a policy from a given state.\"\"\"\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        # Solve the linear system for value function\n",
        "        threshold = 1e-10\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.n_states):\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "\n",
        "                new_v = 0\n",
        "                for s_next in range(self.n_states):\n",
        "                    new_v += self.env.P[s, a, s_next] * (self.env.R[s, a, s_next] + self.gamma * V[s_next])\n",
        "\n",
        "                V[s] = new_v\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        return V[state].item()\n",
        "\n",
        "    def is_eps_bad(self, state, policy):\n",
        "        \"\"\"Check if policy is ε-bad in given state.\"\"\"\n",
        "        policy_value = self.get_policy_value(state, policy)\n",
        "        return policy_value < self.env.V_star[state] - self.epsilon\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Train UCB-QL algorithm for multiple runs and track ε-bad timesteps.\"\"\"\n",
        "        all_eps_bad_counts = torch.zeros((num_runs, self.T), device=self.device)\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset counters and Q-values for this run\n",
        "            self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "            self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "            # Start at a random state\n",
        "            current_state = torch.randint(0, self.n_states, (1,), device=self.device).item()\n",
        "\n",
        "            # Cumulative count of ε-bad timesteps\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get policy (greedy with respect to Q)\n",
        "                policy = self.get_greedy_policy()\n",
        "\n",
        "                # Check if policy is ε-bad\n",
        "                if self.is_eps_bad(current_state, policy):\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative number of ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # Choose action using UCB rule\n",
        "                action = self.get_action(current_state, t)\n",
        "\n",
        "                # Take action and observe next state and reward\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Update Q-value (using standard Q-learning update)\n",
        "                best_next_q = torch.max(self.Q[next_state])\n",
        "                lr = 1.0 / torch.sqrt(self.N[current_state, action].float())  # Learning rate\n",
        "\n",
        "                # Q-learning update\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        # Move results back to CPU for plotting\n",
        "        return all_eps_bad_counts.cpu().numpy()\n",
        "\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    \"\"\"Plot sample path of n(t) for a single run.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    \"\"\"Plot average n(t) with 95% confidence intervals.\"\"\"\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    # Calculate 95% confidence interval\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over {n_runs} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('average_run_V2.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Check and print GPU availability\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000\n",
        "\n",
        "    # Create environment with GPU support\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = RiverSwim(gamma=gamma, device=device)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    # single_run_counts = agent.train(num_runs=1)[0]\n",
        "    # plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=100)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "f6OD-WuTA11x",
        "outputId": "bb15cd2d-369b-47c1-d2fc-6707817bd424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining runs:   0%|          | 0/100 [00:00<?, ?it/s]<ipython-input-6-7504e47ca934>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_term = torch.log(torch.tensor(self.n_states * self.n_actions * torch.log(torch.tensor(t + 1)) / self.delta))\n",
            "Training runs: 100%|██████████| 100/100 [12:59<00:00,  7.80s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V3"
      ],
      "metadata": {
        "id": "vLKssd9waWmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        # Set device (GPU if available, otherwise CPU)\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Use torch tensors and move to device\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "\n",
        "        # Initialize transition probabilities and rewards\n",
        "        self.initialize_mdp()\n",
        "\n",
        "        # Calculate optimal values\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        # Use torch tensors and move to device\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:  # leftmost state\n",
        "                self.P[s, 0, s] = 1.0  # stay in the same state\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7  # move left with high probability\n",
        "                self.P[s, 0, s] = 0.3    # small chance to stay\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:  # rightmost state\n",
        "                self.P[s, 1, s] = 0.7    # high probability to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # small chance to move left\n",
        "                self.R[s, 1, s] = 1.0    # reward for staying in rightmost state\n",
        "            elif s == 0:  # leftmost state\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.4    # decent chance to stay\n",
        "                self.R[s, 1, s] = 0.05   # small reward for left state\n",
        "            else:  # middle states\n",
        "                self.P[s, 1, s+1] = 0.6  # move right with some probability\n",
        "                self.P[s, 1, s] = 0.1    # small chance to stay\n",
        "                self.P[s, 1, s-1] = 0.3  # decent chance to move left\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        \"\"\"Calculate optimal state values using vectorized value iteration.\"\"\"\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        # Vectorized Q-value calculation\n",
        "        while True:\n",
        "            # Compute Q-values for all states and actions in one go\n",
        "            # Dimensions: (states, actions)\n",
        "            Q_values = torch.einsum('sas,sas->sa', self.P, self.R + self.gamma * V)\n",
        "\n",
        "            # Find max Q-value for each state\n",
        "            new_V = Q_values.max(dim=1).values\n",
        "\n",
        "            # Check convergence\n",
        "            delta = torch.abs(new_V - V).max()\n",
        "\n",
        "            # Update V\n",
        "            V = new_V\n",
        "\n",
        "            if delta < 1e-3:\n",
        "                break\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "        # Calculate optimal policy\n",
        "        self.pi_star = Q_values.argmax(dim=1)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Take a step in the environment given state and action.\"\"\"\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2000000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize Q-values, visit counts, and bonus parameters using torch tensors\n",
        "        self.Q = torch.zeros((self.n_states, self.n_actions), device=self.device)\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Set horizon based on discount and epsilon as suggested\n",
        "        self.H = int(1/(1-gamma) * np.log(1/epsilon))\n",
        "\n",
        "    def get_ucb_bonus(self, t):\n",
        "        \"\"\"Calculate UCB bonus for all state-action pairs.\"\"\"\n",
        "        # Avoid division by zero\n",
        "        safe_N = torch.clamp(self.N, min=1)\n",
        "\n",
        "        # Log term\n",
        "        log_term = torch.log(\n",
        "            self.n_states * self.n_actions * torch.log(torch.tensor(t + 1, device=self.device)) / self.delta\n",
        "        )\n",
        "\n",
        "        # Vectorized bonus calculation\n",
        "        bonus = torch.sqrt((self.H / safe_N) * log_term)\n",
        "        return bonus\n",
        "\n",
        "    def get_policy_value(self, policy):\n",
        "        \"\"\"Calculate value of a policy using matrix operations.\"\"\"\n",
        "        # Create identity matrix to help with policy evaluation\n",
        "        I = torch.eye(self.n_states, device=self.device)\n",
        "\n",
        "        # Construct policy transition matrix and reward matrix\n",
        "        P_pi = torch.zeros((self.n_states, self.n_states), device=self.device)\n",
        "        R_pi = torch.zeros(self.n_states, device=self.device)\n",
        "\n",
        "        for s in range(self.n_states):\n",
        "            a = policy[s]\n",
        "            P_pi[s] = self.env.P[s, a]\n",
        "            R_pi[s] = torch.sum(self.env.P[s, a] * self.env.R[s, a])\n",
        "\n",
        "        # Solve (I - γP)V = R using matrix inverse\n",
        "        V = torch.linalg.solve(I - self.gamma * P_pi, R_pi)\n",
        "\n",
        "        return V\n",
        "\n",
        "    def train(self, num_runs=1):\n",
        "        \"\"\"Vectorized training of UCB-QL algorithm.\"\"\"\n",
        "        # Preallocate tensor for tracking ε-bad timesteps\n",
        "        all_eps_bad_counts = torch.zeros((num_runs, self.T), device=self.device)\n",
        "\n",
        "        for run in tqdm(range(num_runs), desc=\"Training runs\"):\n",
        "            # Reset for this run\n",
        "            self.Q.zero_()\n",
        "            self.N.zero_()\n",
        "\n",
        "            # Start at random state\n",
        "            current_state = torch.randint(0, self.n_states, (1,), device=self.device).item()\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in range(self.T):\n",
        "                # Get current greedy policy\n",
        "                policy = self.Q.argmax(dim=1)\n",
        "\n",
        "                # Vectorized policy value calculation\n",
        "                policy_values = self.get_policy_value(policy)\n",
        "\n",
        "                # Check if policy is ε-bad for current state\n",
        "                is_eps_bad = policy_values[current_state] < self.env.V_star[current_state] - self.epsilon\n",
        "                if is_eps_bad:\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Record cumulative ε-bad timesteps\n",
        "                all_eps_bad_counts[run, t] = eps_bad_count\n",
        "\n",
        "                # UCB action selection (includes bonus)\n",
        "                ucb_values = self.Q[current_state] + self.get_ucb_bonus(t)[current_state]\n",
        "                action = ucb_values.argmax().item()\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "\n",
        "                # Update visit count\n",
        "                self.N[current_state, action] += 1\n",
        "\n",
        "                # Learning rate\n",
        "                lr = 1.0 / torch.sqrt(self.N[current_state, action].float())\n",
        "\n",
        "                # Q-learning update\n",
        "                best_next_q = self.Q[next_state].max()\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * best_next_q - self.Q[current_state, action])\n",
        "\n",
        "                # Move to next state\n",
        "                current_state = next_state\n",
        "\n",
        "        # Move results back to CPU for plotting\n",
        "        return all_eps_bad_counts.cpu().numpy()\n",
        "\n",
        "# Plotting functions remain the same as in previous version\n",
        "def plot_single_run(eps_bad_counts):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(eps_bad_counts)\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title('Sample Path of Cumulative ε-bad Timesteps')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('single_run_V3.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_average_with_ci(all_eps_bad_counts):\n",
        "    mean_counts = np.mean(all_eps_bad_counts, axis=0)\n",
        "    std_counts = np.std(all_eps_bad_counts, axis=0)\n",
        "    n_runs = all_eps_bad_counts.shape[0]\n",
        "\n",
        "    ci_95 = 1.96 * std_counts / np.sqrt(n_runs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_counts, label='Average n(t)')\n",
        "    plt.fill_between(np.arange(len(mean_counts)), mean_counts - ci_95, mean_counts + ci_95,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "\n",
        "    plt.xlabel('Timestep (t)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps n(t)')\n",
        "    plt.title(f'Average Cumulative ε-bad Timesteps (over 100 runs)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('FINAL_RUN_V3.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Check and print GPU availability\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Set parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "    T = 2000000\n",
        "\n",
        "    # Create environment with GPU support\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = RiverSwim(gamma=gamma, device=device)\n",
        "\n",
        "    # Create agent\n",
        "    agent = UCBQLearning(env, epsilon=epsilon, delta=delta, gamma=gamma, T=T)\n",
        "\n",
        "    # Run single iteration for part (i)\n",
        "    # single_run_counts = agent.train(num_runs=1)[0]\n",
        "    # plot_single_run(single_run_counts)\n",
        "\n",
        "    # Run multiple iterations for part (ii)\n",
        "    all_eps_bad_counts = agent.train(num_runs=10)\n",
        "    plot_average_with_ci(all_eps_bad_counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Gk74_9fhaXp7",
        "outputId": "e4ccf4b2-fe8d-4ac6-a113-be62cb4d9536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training runs: 100%|██████████| 10/10 [05:21<00:00, 32.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V4"
      ],
      "metadata": {
        "id": "mTuBnVCnw_DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92, device=None):\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "        self.states = torch.arange(self.n_states, device=self.device)\n",
        "        self.actions = torch.tensor([0, 1], device=self.device)\n",
        "        self.initialize_mdp()\n",
        "        self.calculate_optimal_values()\n",
        "\n",
        "    def initialize_mdp(self):\n",
        "        self.P = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "        self.R = torch.zeros((self.n_states, self.n_actions, self.n_states), device=self.device)\n",
        "\n",
        "        # Action 0 (left/downstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == 0:\n",
        "                self.P[s, 0, s] = 1.0\n",
        "            else:\n",
        "                self.P[s, 0, s-1] = 0.7\n",
        "                self.P[s, 0, s] = 0.3\n",
        "\n",
        "        # Action 1 (right/upstream)\n",
        "        for s in range(self.n_states):\n",
        "            if s == self.n_states - 1:\n",
        "                self.P[s, 1, s] = 0.7\n",
        "                self.P[s, 1, s-1] = 0.3\n",
        "                self.R[s, 1, s] = 1.0\n",
        "            elif s == 0:\n",
        "                self.P[s, 1, s+1] = 0.6\n",
        "                self.P[s, 1, s] = 0.4\n",
        "                self.R[s, 1, s] = 0.05\n",
        "            else:\n",
        "                self.P[s, 1, s+1] = 0.6\n",
        "                self.P[s, 1, s] = 0.1\n",
        "                self.P[s, 1, s-1] = 0.3\n",
        "\n",
        "    def calculate_optimal_values(self):\n",
        "        V = torch.zeros(self.n_states, device=self.device)\n",
        "        while True:\n",
        "            Q = torch.einsum('sas,sas->sa', self.P, self.R + self.gamma * V)\n",
        "            new_V = Q.max(dim=1).values\n",
        "            if torch.abs(new_V - V).max() < 1e-3:\n",
        "                break\n",
        "            V = new_V\n",
        "        self.V_star = V\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state_probs = self.P[state, action]\n",
        "        next_state = torch.multinomial(next_state_probs, 1).item()\n",
        "        reward = self.R[state, action, next_state]\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQLearning:\n",
        "    def __init__(self, env, epsilon=0.13, delta=0.05, gamma=0.92, T=2_000_000):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.T = T\n",
        "        self.n_states = env.n_states\n",
        "        self.n_actions = env.n_actions\n",
        "        self.device = env.device\n",
        "\n",
        "        # Initialize parameters as per paper\n",
        "        self.H = int(np.ceil(np.log(3 / (epsilon * (1 - gamma))) / (1 - gamma)))  # Correct H\n",
        "        self.Q = torch.full((self.n_states, self.n_actions), 1/(1 - gamma), device=self.device)  # Optimistic initialization\n",
        "        self.N = torch.zeros((self.n_states, self.n_actions), dtype=torch.long, device=self.device)\n",
        "\n",
        "    def get_ucb_bonus(self, t):\n",
        "        \"\"\"Vectorized UCB bonus calculation as per Wang et al.\"\"\"\n",
        "        safe_N = torch.clamp(self.N, min=1)\n",
        "        log_term = torch.log((self.n_states * self.n_actions * torch.log(torch.tensor(t + 1, device=self.device)) / self.delta))\n",
        "        return torch.sqrt((self.H / safe_N) * log_term)\n",
        "\n",
        "    def train(self, num_runs=100):\n",
        "        all_eps_bad = torch.zeros((num_runs, self.T // 1000), device=self.device)  # Downsampled storage\n",
        "\n",
        "        for run in range(num_runs):\n",
        "            self.Q.fill_(1/(1 - self.gamma))  # Reset Q\n",
        "            self.N.zero_()\n",
        "            current_state = torch.randint(0, self.n_states, (1,)).item()\n",
        "            eps_bad_count = 0\n",
        "\n",
        "            for t in tqdm(range(self.T), desc=f\"Run {run+1}/{num_runs}\", leave=False):\n",
        "                # Get UCB values for current state\n",
        "                ucb = self.Q[current_state] + self.get_ucb_bonus(t)[current_state]\n",
        "                action = ucb.argmax().item()\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward = self.env.step(current_state, action)\n",
        "                self.N[current_state, action] += 1\n",
        "                k = self.N[current_state, action]\n",
        "\n",
        "                # Q-learning update with correct learning rate\n",
        "                lr = (self.H + 1) / (self.H + k)  # Paper's α_k\n",
        "                self.Q[current_state, action] += lr * (reward + self.gamma * self.Q[next_state].max() - self.Q[current_state, action])\n",
        "\n",
        "                # Check ε-bad using UCB values (no full policy evaluation)\n",
        "                current_V_ucb = self.Q[current_state].max() + self.get_ucb_bonus(t)[current_state].max()\n",
        "                if current_V_ucb < self.env.V_star[current_state] - self.epsilon:\n",
        "                    eps_bad_count += 1\n",
        "\n",
        "                # Downsample: store every 1000 steps\n",
        "                if t % 1000 == 0:\n",
        "                    all_eps_bad[run, t // 1000] = eps_bad_count\n",
        "\n",
        "                current_state = next_state\n",
        "\n",
        "        return all_eps_bad.cpu().numpy()\n",
        "\n",
        "def plot_results(data, T=2_000_000):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(0, T, 1000) // 1000  # Plot in units of 1000 steps\n",
        "    plt.plot(x, np.mean(data, axis=0), label='Average n(t)')\n",
        "    plt.fill_between(x,\n",
        "                     np.mean(data, axis=0) - 1.96 * np.std(data, axis=0)/np.sqrt(data.shape[0]),\n",
        "                     np.mean(data, axis=0) + 1.96 * np.std(data, axis=0)/np.sqrt(data.shape[0]),\n",
        "                     alpha=0.3, label='95% CI')\n",
        "    plt.xlabel('Timestep (t, in thousands)')\n",
        "    plt.ylabel('Cumulative ε-bad timesteps')\n",
        "    plt.title(f'UCB-QL Performance (T={T//1_000}K, {data.shape[0]} runs)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig('corrected_results.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "    env = RiverSwim(gamma=0.92, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    agent = UCBQLearning(env, T=2000000)\n",
        "    data = agent.train(num_runs=10)\n",
        "    plot_results(data, 2000000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "1VGztaQ8xAa3",
        "outputId": "d2e22330-c6d0-483c-b3ab-ad1605d5484c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V5"
      ],
      "metadata": {
        "id": "hZ6eu8GqjnEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, gamma=0.92):\n",
        "        self.n_states = 5\n",
        "        self.n_actions = 2\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Precompute transition probabilities and rewards\n",
        "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        # Compute optimal values using value iteration\n",
        "        self.compute_optimal_values()\n",
        "\n",
        "    def compute_optimal_values(self):\n",
        "        # Value iteration to compute optimal values\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "\n",
        "        while True:\n",
        "            V_new = np.zeros(self.n_states)\n",
        "            for s in range(self.n_states):\n",
        "\n",
        "                # Consider both actions\n",
        "                values = []\n",
        "\n",
        "                for a in range(self.n_actions):\n",
        "                    # Expected value for this action\n",
        "                    v = 0\n",
        "                    if s == 4 and a == 1:  # Rightmost state, upstream action\n",
        "                        v = 1 + self.gamma * V[4]\n",
        "                    elif s == 0 and a == 1:  # Leftmost state, upstream action\n",
        "                        # Mostly stay, small chance to move right\n",
        "                        v = 0.05 + 0.6 * self.gamma * V[1]\n",
        "                    else:\n",
        "                        # General transition probabilities\n",
        "                        if a == 0:  # Left/downstream action\n",
        "                            v = 0.7 * self.gamma * V[max(0, s-1)] + 0.3 * self.gamma * V[s]\n",
        "                        else:  # Right/upstream action\n",
        "                            if s == self.n_states - 2:\n",
        "                                v = 0.6 * self.gamma * V[s+1] + 0.1 * self.gamma * V[s] + 0.3 * self.gamma * V[s-1]\n",
        "                            else:\n",
        "                                v = 0.6 * self.gamma * V[s+1] + 0.1 * self.gamma * V[s] + 0.3 * self.gamma * V[s-1]\n",
        "                    values.append(v)\n",
        "                V_new[s] = max(values)\n",
        "\n",
        "            # Check convergence\n",
        "            if np.max(np.abs(V - V_new)) < 1e-2:\n",
        "                break\n",
        "            V = V_new\n",
        "\n",
        "        self.V_star = V\n",
        "\n",
        "    def reset(self):\n",
        "        # Uniform distribution over states\n",
        "        return np.random.randint(0, self.n_states)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Stochastic transitions\n",
        "        if state == 4 and action == 1:  # Rightmost state, upstream action\n",
        "            return 4, 1\n",
        "\n",
        "        # Transition probabilities\n",
        "        if action == 0:  # Left/downstream\n",
        "            trans_probs = [0.7, 0.3] if state > 0 else [0, 1]\n",
        "            next_state = max(0, state-1) if np.random.random() < 0.7 else state\n",
        "            reward = 0\n",
        "        else:  # Right/upstream\n",
        "            if state == 0:\n",
        "                trans_probs = [0.6, 0.4]\n",
        "                next_state = 1 if np.random.random() < 0.6 else 0\n",
        "                reward = 0.05\n",
        "            elif state == 4:\n",
        "                trans_probs = [0.3, 0.7]\n",
        "                next_state = 3 if np.random.random() < 0.3 else 4\n",
        "                reward = 1\n",
        "            else:\n",
        "                trans_probs = [0.3, 0.1, 0.6]\n",
        "                r = np.random.random()\n",
        "                if r < 0.3:\n",
        "                    next_state = state-1\n",
        "                elif r < 0.4:\n",
        "                    next_state = state\n",
        "                else:\n",
        "                    next_state = state+1\n",
        "                reward = 0\n",
        "\n",
        "        return next_state, reward\n",
        "\n",
        "class UCBQL:\n",
        "    def __init__(self, n_states, n_actions, gamma, epsilon, delta, T):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.T = T\n",
        "\n",
        "        # Horizon calculation\n",
        "        self.H = int(1 / (1 - gamma) * np.log(1 / epsilon))\n",
        "\n",
        "        # Initialize Q-values optimistically\n",
        "        self.Q = np.ones((n_states, n_actions)) / (1 - gamma)\n",
        "        self.N = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def get_confidence_bonus(self, state, action, t):\n",
        "        # UCB bonus as per the paper\n",
        "        N_sa = max(1, self.N[state, action])\n",
        "        log_term = np.log(self.n_states * self.n_actions * np.log(t + 1) / self.delta)\n",
        "        return np.sqrt(self.H / N_sa * log_term)\n",
        "\n",
        "    def choose_action(self, state, t):\n",
        "        # UCB action selection\n",
        "        bonus_values = self.Q[state, :] + [self.get_confidence_bonus(state, a, t)\n",
        "                                           for a in range(self.n_actions)]\n",
        "        return np.argmax(bonus_values)\n",
        "\n",
        "    def update(self, state, action, next_state, reward, t):\n",
        "        # Update visit count\n",
        "        self.N[state, action] += 1\n",
        "        k = self.N[state, action]\n",
        "\n",
        "        # Learning rate as suggested in the paper\n",
        "        lr = (self.H + 1) / (self.H + k)\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state, action] += lr * (reward + self.gamma * np.max(self.Q[next_state, :]) -\n",
        "                                       self.Q[state, action])\n",
        "\n",
        "def run_ucb_ql_experiment(T=2000000, n_runs=5):\n",
        "    # Experiment parameters\n",
        "    gamma = 0.92\n",
        "    epsilon = 0.13\n",
        "    delta = 0.05\n",
        "\n",
        "    # Store cumulative epsilon-bad time steps\n",
        "    all_n_t = []\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        env = RiverSwim(gamma)\n",
        "        agent = UCBQL(n_states=5, n_actions=2, gamma=gamma,\n",
        "                      epsilon=epsilon, delta=delta, T=T)\n",
        "\n",
        "        # Track epsilon-bad time steps\n",
        "        n_t = np.zeros(T)\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(T):\n",
        "            # Choose action with UCB exploration\n",
        "            action = agent.choose_action(state, t+1)\n",
        "\n",
        "            # Take step\n",
        "            next_state, reward = env.step(state, action)\n",
        "\n",
        "            # Check if time step is epsilon-bad\n",
        "            # Compare UCB Q-value estimate with true optimal value\n",
        "            ucb_value = agent.Q[state, action] + agent.get_confidence_bonus(state, action, t+1)\n",
        "            if ucb_value < env.V_star[state] - epsilon:\n",
        "                n_t[t] = 1\n",
        "\n",
        "            # Update Q-values\n",
        "            agent.update(state, action, next_state, reward, t+1)\n",
        "\n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "\n",
        "        # Store cumulative epsilon-bad time steps\n",
        "        all_n_t.append(np.cumsum(n_t))\n",
        "\n",
        "    # Compute mean and confidence intervals\n",
        "    mean_n_t = np.mean(all_n_t, axis=0)\n",
        "    std_n_t = np.std(all_n_t, axis=0)\n",
        "    confidence_interval = 1.96 * std_n_t / np.sqrt(n_runs)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(mean_n_t, label='Mean Cumulative ε-Bad Time Steps')\n",
        "    plt.fill_between(range(T),\n",
        "                     mean_n_t - confidence_interval,\n",
        "                     mean_n_t + confidence_interval,\n",
        "                     alpha=0.3, label='95% Confidence Interval')\n",
        "    plt.title('UCB Q-Learning: Cumulative ε-Bad Time Steps')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Cumulative ε-Bad Time Steps')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mean_n_t, confidence_interval\n",
        "\n",
        "# Run the experiment\n",
        "mean_n_t, confidence_interval = run_ucb_ql_experiment()"
      ],
      "metadata": {
        "id": "HhK7LtL3joLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}