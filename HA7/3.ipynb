{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from HA7_gridworld import Four_Room_Teleportation, display_4room_policy\n",
    "\n",
    "def value_iteration_average_reward(P, R, epsilon=1e-6, max_iter=10000):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration for Average-Reward MDP\n",
    "    \n",
    "    Parameters:\n",
    "    - P: Transition probability matrix (states x actions x states)\n",
    "    - R: Reward matrix (states x actions)\n",
    "    - epsilon: Convergence threshold\n",
    "    - max_iter: Maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - policy: Optimal policy\n",
    "    - g_star: Optimal average reward (gain)\n",
    "    - b_star: Bias function\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Initialize bias and gain\n",
    "    b = np.zeros(n_states)\n",
    "    g = 0\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Store previous values for convergence check\n",
    "        prev_b = b.copy()\n",
    "        prev_g = g\n",
    "        \n",
    "        # Compute Q-values\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # Compute expected value considering current bias\n",
    "                Q[s, a] = R[s, a] - g + np.sum(P[s, a] * b)\n",
    "        \n",
    "        # Update bias and gain\n",
    "        # Maximize Q-values while maintaining current gain\n",
    "        for s in range(n_states):\n",
    "            b[s] = np.max(Q[s])\n",
    "        \n",
    "        # Center the bias function\n",
    "        b -= np.mean(b)\n",
    "        \n",
    "        # Check for convergence\n",
    "        g_diff = np.abs(g - np.mean(np.max(Q, axis=1)))\n",
    "        b_diff = np.max(np.abs(b - prev_b))\n",
    "        \n",
    "        # Update gain\n",
    "        g = np.mean(np.max(Q, axis=1))\n",
    "        \n",
    "        # Convergence criteria\n",
    "        if g_diff < epsilon and b_diff < epsilon:\n",
    "            break\n",
    "    \n",
    "    # Compute optimal policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return policy, g, b\n",
    "\n",
    "def main():\n",
    "    # Create the grid-world environment\n",
    "    env = Four_Room_Teleportation()\n",
    "    \n",
    "    # Perform Value Iteration\n",
    "    policy, g_star, b_star = value_iteration_average_reward(\n",
    "        env.P, env.R, epsilon=1e-6\n",
    "    )\n",
    "    \n",
    "    # Calculate the span of the bias function\n",
    "    bias_span = np.max(b_star) - np.min(b_star)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Optimal Gain (g*): {g_star}\")\n",
    "    print(f\"Bias Function Span: {bias_span}\")\n",
    "    \n",
    "    # Visualize the policy\n",
    "    policy_matrix = display_4room_policy(policy)\n",
    "    \n",
    "    # Print policy matrix\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy_matrix:\n",
    "        print(\" \".join(row))\n",
    "    \n",
    "    # Return the results as a dictionary for additional processing if needed\n",
    "    return {\n",
    "        'policy': policy,\n",
    "        'optimal_gain': g_star,\n",
    "        'bias_span': bias_span,\n",
    "        'policy_matrix': policy_matrix\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
